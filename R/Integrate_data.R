# write_log <- function(message, level="INFO") {
#
#   path = 'log.txt'
#   create.logger(path, level = 2)
#   appender = file_appender(path)
#   # hardcode输出样式
#   appender(level," ", message)
# }

rd_data <- function(file_path,smp_name){
  tryCatch({
    anno <- fread(file_path,fill=T)
    if (!"TYPE" %in% colnames(anno)){
      return(NULL)
    }

    anno <- anno %>%
      dplyr::distinct(TYPE,REF,ALT,POS,.keep_all = T) %>%
      as.data.table()
    cols_to_replace <- c("FTYPE","STRAND","NT_POS","AA_POS","EFFECT","LOCUS_TAG",
                         "GENE","PRODUCT")

    anno[, (cols_to_replace) := lapply(.SD, function(x) ifelse(is.na(x), "", x)), .SDcols = cols_to_replace]
    anno[, "TYPE" := lapply(.SD, function(x) ifelse(x=="mnp", "complex", x)), .SDcols = "TYPE"]
    anno[, POS := as.integer(POS)]
    anno[,EVIDENCE := NULL]  # 改用data.table语法，快了将近8倍速度

    anno[,smp_name] <- 1 # dplyr版本传参改名：mutate_(.dots= setNames(list(varval), varname))
    return(as.data.frame(anno))
  },error=function(e){
    stop(paste0("ERROR occured while read ",smp_name))
    })

}

# 定义一个函数，分批次合并
batch_merge <- function(data_list, batch_size = 2) {
  # 将数据框列表分成每 batch_size 一组
  batches <- split(data_list, ceiling(seq_along(data_list) / batch_size))

  # 对每一组进行合并
  merged_batches <- map(batches, function(batch) {
    purrr::reduce(batch, function(x, y) suppressMessages(full_join(x, y)))
    # purrr::reduce(batch, function(x, y) suppressMessages(merge(x, y)))
  },.progress = T)
  return(merged_batches)
}

combine_csv <- function(csv_list){
  while (length(csv_list)>2){

    message("Number of files is ", length(csv_list), ", combining files to accelerate the procedure")

    csv_list <- batch_merge(csv_list)
  }
  final_df <- purrr::reduce(csv_list, function(x, y) {
    suppressMessages(full_join(x, y))
  })

  return(final_df)
}

as.list_spM <- function(data.use,chun_size=20000000,sparseClass="dgCMatrix",by) {
  if (by=="row"){
    pblapply( split(seq(nrow(data.use)), (seq(nrow(data.use))-1) %/% chun_size ) ,
              function(nx) {
                tmp_data <- data.use[nx,]
                tmp_data[is.na(tmp_data)] <- 0
                switch(sparseClass,
                       dgTMatrix = {as(as.matrix(tmp_data),"dgTMatrix")},
                       dgCMatrix = {as(as.matrix(tmp_data),"dgCMatrix")},
                       dgRMatrix = {as(as.matrix(tmp_data),"dgRMatrix")},
                       stop("Enter one of the three types of sparse matrix(\"dgTMatrix\",\"dgCMatrix\",\"dgRMatrix\")") ) }) %>%
                Reduce(rbind,.)
  } else if (by=="col"){
    pblapply( split(seq(ncol(data.use)), (seq(ncol(data.use))-1) %/% chun_size ) ,
              function(nx) {
                tmp_data <- data.use[,nx]
                tmp_data[is.na(tmp_data)] <- 0
                switch(sparseClass,
                       dgTMatrix = {as(as.matrix(tmp_data),"dgTMatrix")},
                       dgCMatrix = {as(as.matrix(tmp_data),"dgCMatrix")},
                       dgRMatrix = {as(as.matrix(tmp_data),"dgRMatrix")},
                       stop("Enter one of the three types of sparse matrix(\"dgTMatrix\",\"dgCMatrix\",\"dgRMatrix\")") ) }) %>%
      purrr::reduce(cbind,.)} else {
        stop("by must be 'row' or 'col'")
      }
}




#' Integrate SNP result generated by snippy software
#'
#' @param file_path a vector of file path for SNP data. The SNP data should be generated by snippy and resulted in the csv format.
#' @param smp_name name of each SNP sample
#' @param chunk_size chunk size for each batch. This parameter is used to prevent
#' too many samples integrated into one batch, which will require for larger RAM.
#' @param progress_bar whether to show progress bar
#' @param nworkers number of workers to integrate data.
#'
#' @export
#'
IntegrateData <- function(file_path,smp_name,block=50,nworkers=8,
                          output_dir,progress_bar=T,start_block=1){

  future::plan(multisession, workers = nworkers)

  if (!file.exists(output_dir)){dir.create(output_dir)}

  csv_batch <- split(file_path,1:block)
  smp_name_batch <-  split(smp_name,1:block)

  process_block <- function(i){

    message("Process block ",i)
    message("Read in mutation")
    csv_list <- future_map2(csv_batch[[i]],smp_name_batch[[i]],rd_data,.progress = T)

    csv_list <- csv_list[map_lgl(csv_list,~!is.null(.x))]

    csv_list <- csv_list[map_dbl(csv_list,nrow)>0]
    message("Combine mutation")
    res_dt <- combine_csv(csv_list)

    message("Generate snp_anno files")

    snp_anno_list <- res_dt[1:13]

    qsave(snp_anno_list,paste0(output_dir,"/snp_anno_list_",i,".qs"))

    if (length(res_dt)==14){
      snp_data_list <- as(as.matrix(t(res_dt[-c(1:13)])),"dgCMatrix")
    } else{
      snp_data_list <- as.list_spM(t(res_dt[-c(1:13)]),chun_size=100000,by="row")
    }


    qsave(snp_data_list,paste0(output_dir,"/snp_data_list_",i,".qs"),nthreads=8)
  }

  message("Start integrate data")

  if (start_block!= -1){
    res <- map(start_block:block,process_block,.progress = T)
  }

  message("Read snp_anno")
  snp_anno_list <- map(list.files(output_dir,full.names = T,recursive = T,pattern = "snp_anno_list_"),
                       ~qread(.x,nthreads=8),.progress=T)

  message("Read snp_data")
  snp_data_list <- map(list.files(output_dir,full.names = T,recursive = T,pattern = "snp_data_list_"),
                       ~qread(.x,nthreads=8),.progress=T)

  # message("Generate snp_anno files")
  # snp_anno_list <- map(res,~.x[1:13],.progress = T)
  # saveRDS(snp_anno_list,paste0(output_dir,"/snp_anno_list.rds"))
  #
  # message("Generate snp_data files")
  # snp_data_list <- map(res,~as.list_spM(t(.x[-c(1:13)]),chun_size=100000,by="row"),.progress = T)
  # saveRDS(snp_data_list,paste0(output_dir,"/snp_data_list.rds"))

  message("Construct partial MutationObject")
  all_data <- map2(snp_anno_list,snp_data_list,CreateSNPObj,.progress = progress_bar)

  while (length(all_data)>1){

    all_data <- MergeMutationObj(all_data)

  }

  if (class(all_data)=="list"){
    all_data <- all_data[[1]]
  }


  qsave(all_data$snp_list$snp_anno,file = paste0(output_dir,"/snp_anno.qs"),nthreads = 8)
  qsave(all_data$snp_list$snp_data,file = paste0(output_dir,"/snp_data.qs"),nthreads = 8)

  return(all_data)

}


